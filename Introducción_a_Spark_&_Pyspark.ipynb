{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8ZUAlAWCRvtD",
        "R7IHfTpTQ38i",
        "DsAocF7UrU9z",
        "GhDBezDJrywP",
        "Lo5bA5dMRUte"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/N34R20/PySpark/blob/main/Introduccio%CC%81n_a_Spark_%26_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://th.bing.com/th/id/R.38bf6a6c11d543d1015e724f002798a4?rik=25ujuy3w4NDXtg&pid=ImgRaw&r=0' width='210'/>"
      ],
      "metadata": {
        "id": "mWMdVoKiECo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apache Spark** es un open-source framework para ejecutar codigo de manera paralela en maquinas diferentes, esto ultimo se conoce como procesamiento distribuido en un cluster. Esta casi en su totalidad escrito en Scala pero podemos usar Spark escribiendo codigo en Python con la libreria PySpark."
      ],
      "metadata": {
        "id": "G0IKjmYd0aEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Stack\n",
        "---"
      ],
      "metadata": {
        "id": "d4RFBh-amYvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/engcarlosperezmolero/resources_and_tools/blob/main/imgs/pyspark-class/spark-stack.png?raw=true' />"
      ],
      "metadata": {
        "id": "9gQeC6zVn7ZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conceptos Claves\n",
        "---"
      ],
      "metadata": {
        "id": "zRutChA0n3Ar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SparkSession (tambien conocido como ```spark```)\n",
        "Una clase definida en el paquete de ```pyspark.sql```. Esta clase es el punto de acceso unificado para programar Spark usando la API Estructurada (DataFrame y Dataset). Se usa normalmente para crear Dataframes, registrar Dataframes como tablas, ejecutar SQL sobre tablas y leer archivos parquet. Desde esta clase puedes acceder tambien al SparkContext.\n",
        "\n",
        "Se le denomina \"unificado\" porque agrupa los siguientes puntos de acceso a otras funcionalidades de Spark:\n",
        "- Spark Context\n",
        "- SQL Context\n",
        "- Hive Context\n",
        "- Streaming Context\n",
        "- Spark Configuration\n"
      ],
      "metadata": {
        "id": "zt8AE4gUpCDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SparkContext (tambien conocido como ```sc```)\n",
        "Una clase definida en el paquete de ```pyspark``` y representa la conexion al Spark Cluster. Es el punto de acceso principal para usar el motor de Spark. Mantiene una conexion con el \"cluster manager\" de Spark y puede ser usado para crear RDD y hacer broadcasting de variables en ese cluster. Todas las aplicaciones de Spark (incluyendo PySpark Shell y programas de Python aislados) se ejecutan como un conjunto de procesos independientes. Estos procesos son coordinados por el SparkContext  en un programa driver.\n"
      ],
      "metadata": {
        "id": "cj9J9wyqESez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Driver (archivo .py o notebook .ipynb)\n",
        "Para enviar (submit) un programa de Python a Spark, necesitars escribir un programa Driver con algun Lenguaje que proporcione la conexion a spark (como PySpark). Este programa esta a cargo de crear el SparkContext, RDD y Dataframes.\n"
      ],
      "metadata": {
        "id": "lyT-QRZXEYAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Worker (Recursos = Procesadores | Memoria RAM)\n",
        "En el entorno del cluster de Spark, existen dos tipos de nodos:\n",
        "- 1 Master (o 2 para hablar de alta disponibilidad, a veces mas)\n",
        "- Conjunto de Workers.\n",
        "\n",
        "Worker es cualquier nodo capaz de ejecutar programas en el cluster. Si un proceso es disparado para cierta aplicacion, entonces esta aplicacion adquiere executors y workers, los cuales llevan a cabo la ejecucion de las tareas de la aplicacion de Spark.\n",
        "\n",
        "Tambien se conocen como nodo Slave (Esclavo).\n"
      ],
      "metadata": {
        "id": "6drbOzr-EcPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Executor (procesos dentro de los Workers)\n",
        "Son los procesos que existen dentro de los Workers donde se llevan a cabo las tareas designadas. Cada worker genera subprocesos de python donde se envia el codigo del usuario y los datos para ser procesados.\n"
      ],
      "metadata": {
        "id": "9O4qZv-dEf-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Cluster Manager\n",
        "El nodo master tambien es conocido como el cluster manager. Este se encargara administrar el cluster de servidores necesario para que Spark lleve a cabo las tareas. El cluster manager asigna recursos a cada aplicacion dentro del programa driver. Hay 5 tipos de Cluster Manager aceptados por Spark actualmente:\n",
        "- Standalone: entorno de cluster built-in de Spark.\n",
        "- Mesos: un kernel de Sistemas Distribuidos.\n",
        "- Hadoop YARN (Yet Another Resource Negotiator).\n",
        "- Kubernetes\n",
        "- Amazon EC2"
      ],
      "metadata": {
        "id": "WY2aRUqSEjrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/engcarlosperezmolero/resources_and_tools/blob/main/imgs/pyspark-class/spark-app-arch.png?raw=true' />"
      ],
      "metadata": {
        "id": "oAXLiwD8Bm1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Particiones\n",
        "Para permitir a cada executor realizar trabajos en paralelo, Spark divide los datos en chunks (division logica de la data) llamados particiones. Una particion es la unidad basica de paralelismo en Spark que esta en una maquina fisica (nodo) de tu cluster. Con DataFrames no manipularas particiones manualmente o individualmente, solo especificaras transformaciones de alto nivel. Spark tomara tus instrucciones y se encargara de hacer el trabajo de bajo nivel (usando la RDD API)."
      ],
      "metadata": {
        "id": "saP3IZZIR8yP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDD: Resilient Distributed Dataset\n",
        "---"
      ],
      "metadata": {
        "id": "FtUHqqoTFipA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es literalmente una coleccion de particiones que a su vez es una coleccion de objetos, cualquier clase de objeto puede estar presente, es muy flexible en comparación al Dataframe (el cual es una coleccion de columnas) pero tambien esta caracteristica lo hace mas dificil de operar. Spark tiene 3 tipos de abstracciones de datos RDD es solo 1 de las 3 que existen (siendo las otras dos Dataframes y Datasets).\n",
        "\n",
        "\n",
        "- Se denota como RDD[T], cada elemento tiene tipo T.\n"
      ],
      "metadata": {
        "id": "U6DacAkcm-A_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps4FzalEGVxe",
        "outputId": "8c485167-acab-4354-831c-ee3e6b480651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "# .config(\"spark.sql.repl.eagerEval.enabled\", \"True\")\\\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "TfGDirUdGWed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection = [1, \"two\", 3.0, (\"four\", 4), {\"five\": 5}] # entero, texto, tupla, diccionario\n",
        "collection_rdd = sc.parallelize(collection) # este es el comando que se usa para crear el RDD a partir de una lista"
      ],
      "metadata": {
        "id": "fZ9343kAJBob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(collection_rdd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tjUaG--PzK_",
        "outputId": "9392110e-3c4f-4b42-fc36-7506a4f6eb9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collection_rdd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-zU_lDcP0pJ",
        "outputId": "9301cfa0-2af0-44ba-cd16-b7eece88556a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:287"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collection_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eT52IxMP5TC",
        "outputId": "d58d6327-108a-461a-909c-60fb78e76f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 'two', 3.0, ('four', 4), {'five': 5}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[i for i in dir(collection_rdd) if not i.startswith(\"_\")]"
      ],
      "metadata": {
        "id": "aSwLYw7cP9oI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e61fda-754f-4c5a-ebf3-b979aa184f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aggregate',\n",
              " 'aggregateByKey',\n",
              " 'barrier',\n",
              " 'cache',\n",
              " 'cartesian',\n",
              " 'checkpoint',\n",
              " 'cleanShuffleDependencies',\n",
              " 'coalesce',\n",
              " 'cogroup',\n",
              " 'collect',\n",
              " 'collectAsMap',\n",
              " 'collectWithJobGroup',\n",
              " 'combineByKey',\n",
              " 'context',\n",
              " 'count',\n",
              " 'countApprox',\n",
              " 'countApproxDistinct',\n",
              " 'countByKey',\n",
              " 'countByValue',\n",
              " 'ctx',\n",
              " 'distinct',\n",
              " 'filter',\n",
              " 'first',\n",
              " 'flatMap',\n",
              " 'flatMapValues',\n",
              " 'fold',\n",
              " 'foldByKey',\n",
              " 'foreach',\n",
              " 'foreachPartition',\n",
              " 'fullOuterJoin',\n",
              " 'getCheckpointFile',\n",
              " 'getNumPartitions',\n",
              " 'getResourceProfile',\n",
              " 'getStorageLevel',\n",
              " 'glom',\n",
              " 'groupBy',\n",
              " 'groupByKey',\n",
              " 'groupWith',\n",
              " 'has_resource_profile',\n",
              " 'histogram',\n",
              " 'id',\n",
              " 'intersection',\n",
              " 'isCheckpointed',\n",
              " 'isEmpty',\n",
              " 'isLocallyCheckpointed',\n",
              " 'is_cached',\n",
              " 'is_checkpointed',\n",
              " 'join',\n",
              " 'keyBy',\n",
              " 'keys',\n",
              " 'leftOuterJoin',\n",
              " 'localCheckpoint',\n",
              " 'lookup',\n",
              " 'map',\n",
              " 'mapPartitions',\n",
              " 'mapPartitionsWithIndex',\n",
              " 'mapPartitionsWithSplit',\n",
              " 'mapValues',\n",
              " 'max',\n",
              " 'mean',\n",
              " 'meanApprox',\n",
              " 'min',\n",
              " 'name',\n",
              " 'partitionBy',\n",
              " 'partitioner',\n",
              " 'persist',\n",
              " 'pipe',\n",
              " 'randomSplit',\n",
              " 'reduce',\n",
              " 'reduceByKey',\n",
              " 'reduceByKeyLocally',\n",
              " 'repartition',\n",
              " 'repartitionAndSortWithinPartitions',\n",
              " 'rightOuterJoin',\n",
              " 'sample',\n",
              " 'sampleByKey',\n",
              " 'sampleStdev',\n",
              " 'sampleVariance',\n",
              " 'saveAsHadoopDataset',\n",
              " 'saveAsHadoopFile',\n",
              " 'saveAsNewAPIHadoopDataset',\n",
              " 'saveAsNewAPIHadoopFile',\n",
              " 'saveAsPickleFile',\n",
              " 'saveAsSequenceFile',\n",
              " 'saveAsTextFile',\n",
              " 'setName',\n",
              " 'sortBy',\n",
              " 'sortByKey',\n",
              " 'stats',\n",
              " 'stdev',\n",
              " 'subtract',\n",
              " 'subtractByKey',\n",
              " 'sum',\n",
              " 'sumApprox',\n",
              " 'take',\n",
              " 'takeOrdered',\n",
              " 'takeSample',\n",
              " 'toDF',\n",
              " 'toDebugString',\n",
              " 'toLocalIterator',\n",
              " 'top',\n",
              " 'treeAggregate',\n",
              " 'treeReduce',\n",
              " 'union',\n",
              " 'unpersist',\n",
              " 'values',\n",
              " 'variance',\n",
              " 'withResources',\n",
              " 'zip',\n",
              " 'zipWithIndex',\n",
              " 'zipWithUniqueId']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformaciones y Acciones\n",
        "---"
      ],
      "metadata": {
        "id": "QweAJMujQelE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformaciones (Lazy Evaluated)\n",
        "Ya que en spark las estructuras de datos fundamentales son inmutables, la unica manera de modificar un estructura (RDD o Dataframe) es a traves de una transformacion, esta es una operacion que crea un nuevo RDD o Dataframe.\n",
        "\n",
        "\n",
        "- Entonces transforma un RDD de entrada a un RDD (o varios) de salida.\n",
        "- Una transformacion es basicamente una funcion.\n",
        "- Si un RDD falla durante una transformacion, el linaje de los datos (data lineage descrito por DAG) reconstruye el RDD.\n"
      ],
      "metadata": {
        "id": "ugEg5n1IQjnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_1 = [1, 2, 3, 4]\n",
        "rdd_1 = sc.parallelize(list_1) # creando el RDD[int]"
      ],
      "metadata": {
        "id": "1v9TfQTNQ3xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_2 = rdd_1.map(lambda x: x * 10) # al hacer una transformacion creamos el rdd_2 a partir de rdd_1"
      ],
      "metadata": {
        "id": "gNZNUmljTu9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Narrow Dependencies Transformations (1 a 1)\n",
        "Son aquellas en donde cada particion de entrada despues de transformada correspondera a solo una particion de salida. El ejemplo de arriba (map) es una Narrow Transformation.\n",
        "\n",
        "<img src='https://github.com/engcarlosperezmolero/resources_and_tools/blob/main/imgs/pyspark-class/narrow_transformation.png?raw=true' />\n",
        "\n"
      ],
      "metadata": {
        "id": "Ys2o2jneUcXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wide Dependencies Transformations (Shuffle - 1 a N)\n",
        "Son aquellas en donde al menos una de las particiones de entrada despues de transformada correspondera a varias particiones de salida. Tambien se llama Shuffle porque Spark intercambiara particiones a traves de todo el cluster. Mientras que con las Narrow Transformations Spark automaticamente hara una operacion llamada Pipelining (en donde todo sucede en memoria), para los Shuffles Spark escribira los resultados en disco.\n",
        "\n",
        "<img src='https://github.com/engcarlosperezmolero/resources_and_tools/blob/main/imgs/pyspark-class/wide_transformation.png?raw=true' />"
      ],
      "metadata": {
        "id": "G5XIEfJ2meqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lazy Evaluation\n",
        "Esto es simplemente que Spark esperar para ejecutar una serie de Transformaciones (representadas en forma de DAG) hasta que sea realmente necesario (dado que se realizo una Acción). Gracias a esto Spark es capaz de optimizar el flujo de la data de principio a fin, sin tener nosotros que preocuparnos por eso.\n",
        "- Leer un archivo, transformarlo y luego filtrarlo, no sera mas eficiente que filtrarlo y luego transformarlo."
      ],
      "metadata": {
        "id": "XiTKb5juk6Xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¿Por qué importa saber la diferencia entre ambas transformaciones?"
      ],
      "metadata": {
        "id": "8ZUAlAWCRvtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apariciones_desordenadas = [(\"a\", 1), (\"b\", 3), (\"a\", 2), (\"b\", 5), (\"c\", 2), (\"a\", 2), (\"c\", 3)]\n",
        "rdd_apariciones = sc.parallelize(apariciones_desordenadas) # RDD[(string, int)]"
      ],
      "metadata": {
        "id": "8cj2DeSXme0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_apariciones.saveAsTextFile('./apariciones')"
      ],
      "metadata": {
        "id": "4QkA0bZ2on3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# opcion 1 (no recomendada ya que causa mas suffling)\n",
        "rdd_apariciones.groupByKey().mapValues(lambda values: sum(values)).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31pSS7-2nHmH",
        "outputId": "1e45c62e-c854-4e57-ec39-ad34f0ee6e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 5), ('b', 8), ('c', 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# opcion 2 (casi siempre el reduceByKey sera mas eficiente que el patron de arriba)\n",
        "rdd_apariciones.reduceByKey(lambda x, y: x + y).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfkFuD5yn8Mc",
        "outputId": "794a8d6f-8e0a-4f8a-ce39-b0b22d90629c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 5), ('b', 8), ('c', 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acciones\n",
        "Estas son operaciones o funciones de un RDD que producen valores distintos de un RDD o Dataframe. Son muy importantes ya que estas disparan la ejecucion de los ya construidos lazy RDDs a traves de transformaciones.\n",
        "\n",
        "Las acciones pueden convertir un RDD en cosas tangibles como:\n",
        "- un archivo guardado.\n",
        "- un entero.\n",
        "- un conteo de elementos.\n",
        "- una lista o un diccionario."
      ],
      "metadata": {
        "id": "R7IHfTpTQ38i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_2.count() # produce un entero"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hFLIqn9jVcs",
        "outputId": "830813b3-defb-4757-f7bf-b54fa69021b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_2.collect() # produce una lista de enteros (NO USAR EN RDD GRANDES en servidores de produccion), usar take en su lugar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6Oo0utjjYZS",
        "outputId": "cdc0f0d5-f3ff-473f-ed9a-f45851e66f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 20, 30, 40]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_2.take(2) # DataFrame.take(N) devuelve una lista de las primeras N filas como una lista de objetos Row"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcYFfZH1kE6H",
        "outputId": "915fec67-d018-4efa-f31e-c406122d785a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 20]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_2.getNumPartitions() # un entero"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahXqwFJ1ZYNn",
        "outputId": "915c3407-864d-48b1-ae22-2cc0210dac6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_2.saveAsTextFile(\"./rdd_2\",) # produce archivos guardados en este caso 2"
      ],
      "metadata": {
        "id": "R66TH4vUjZ87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_2.coalesce(1).saveAsTextFile('./rdd_2_coalesce') # produce un archivo"
      ],
      "metadata": {
        "id": "jxqg0MEQZH7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persistencia (cache y persist)"
      ],
      "metadata": {
        "id": "s7YEMQ1d-8lQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es muy util persistir la data cuando se necesita acceder frecuentemente a la misma data, como por ejemplo cuando se esta ejecutando un algoritmo iterativo o cuando quieres realizar distintas acciones sobre un mismo linaje de transformaciones. Esto permite crear un proceso mas eficiente en cuanto a tiempo y costo de recursos.\n",
        "\n",
        "Hay dos maneras de persistir RDDs en Spark:\n",
        "1. ```cache()```\n",
        "2. ```persist()```\n",
        "\n",
        "Existen distintos tipos de almacenamiento para persist (que se importan de ```pyspark.StorageLevel```)\n",
        "\n",
        "\n",
        "Asi mismo si se necesita vaciar el cache de un nodo manualmente entonces se usa el metodo ```unpersist()```"
      ],
      "metadata": {
        "id": "WShQ78t4_EXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark"
      ],
      "metadata": {
        "id": "CNibyrdcQC28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[i for i in dir(pyspark.StorageLevel) if not i.startswith(\"_\")]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B57P9XzMQLaE",
        "outputId": "fcd4c716-ebe8-49c3-9064-e96e7ab95b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DISK_ONLY',\n",
              " 'DISK_ONLY_2',\n",
              " 'DISK_ONLY_3',\n",
              " 'MEMORY_AND_DISK',\n",
              " 'MEMORY_AND_DISK_2',\n",
              " 'MEMORY_AND_DISK_DESER',\n",
              " 'MEMORY_ONLY',\n",
              " 'MEMORY_ONLY_2',\n",
              " 'NONE',\n",
              " 'OFF_HEAP']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_1 = sc.parallelize([\"hola\", \"soy\", \"charly\"])"
      ],
      "metadata": {
        "id": "Tjnb-AxIMjXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def crear_delay(x):\n",
        "    time.sleep(5)\n",
        "    return (x,1) # (\"hola\", 1)\n",
        "\n",
        "rdd_2 = rdd_1.map(lambda x: crear_delay(x))"
      ],
      "metadata": {
        "id": "KyURmt-BMpc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# este resultado tarda 15 segundos aprox porque solo tenemos un nodo (no hay realmente una mejora porque no se puede realizar de manera paralela)\n",
        "%%time\n",
        "rdd_2.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCEOe3-BM8pO",
        "outputId": "d1552175-4d55-4fce-84cc-c8f29f11545d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 86.5 ms, sys: 15.3 ms, total: 102 ms\n",
            "Wall time: 15.4 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hola', 1), ('soy', 1), ('charly', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_2.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6O9xygPQM_ke",
        "outputId": "1ec9fa75-a19a-401d-b608-502130806a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[27] at collect at <timed eval>:1"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ya que dijimos que sea cacheada entonces el resultado de esta celda sera cacheado, para las mismas transformaciones que crean rdd_2\n",
        "%%time\n",
        "rdd_2.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6PHt040NJcU",
        "outputId": "0c26531d-4eff-4cc0-fa88-119b350470f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 69.8 ms, sys: 12.7 ms, total: 82.5 ms\n",
            "Wall time: 15.3 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hola', 1), ('soy', 1), ('charly', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ahora este resultado sera inmediato porque no esta realizando las transformaciones, simplemente agarra rdd_2 directamente de la memoria\n",
        "%%time\n",
        "rdd_2.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugt4qjI2NM_5",
        "outputId": "f50e14f1-04aa-4ac8-a26f-f1ba1ca070a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.35 ms, sys: 720 µs, total: 4.07 ms\n",
            "Wall time: 64.7 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hola', 1), ('soy', 1), ('charly', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_2.is_cached"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQfpSX4sW3Tm",
        "outputId": "bf5f1de2-8a6e-4a89-a9e2-0886e407cb2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# es importante realizar la limpieza de la memoria para no recibir luego un error de OutOfMemory\n",
        "rdd_2.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G032qxy3Nb63",
        "outputId": "116629a1-c72c-415f-f9bf-091f9c15c730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[27] at collect at <timed eval>:1"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ya que el unpersist() lo saca de la memoria entonces ahora vuelve a ejecutar todas las transformaciones\n",
        "%%time\n",
        "rdd_2.collect()"
      ],
      "metadata": {
        "id": "SRnpePQ8Ndx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d193bba-39d7-42ff-e007-1c5aaa916569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 81.1 ms, sys: 8.62 ms, total: 89.7 ms\n",
            "Wall time: 15.3 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hola', 1), ('soy', 1), ('charly', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# realizando la persistencia en otro nivel\n",
        "rdd_2.persist(pyspark.StorageLevel.DISK_ONLY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JDuYnE8NfJP",
        "outputId": "683ad857-51b4-45ff-c3c0-3079b7b75db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[27] at collect at <timed eval>:1"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rdd_2.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzfuQ52ERMHh",
        "outputId": "6e4af514-f398-4949-c0c8-142cd6730215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 72 ms, sys: 13.8 ms, total: 85.8 ms\n",
            "Wall time: 15.3 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hola', 1), ('soy', 1), ('charly', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rdd_2.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSqzJyQrRPLr",
        "outputId": "0997e2ad-5fbd-4155-d8fa-76a5de4b542f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.23 ms, sys: 851 µs, total: 7.09 ms\n",
            "Wall time: 64.1 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hola', 1), ('soy', 1), ('charly', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rdd_2.getStorageLevel())\n",
        "rdd_2.unpersist()\n",
        "print(rdd_2.getStorageLevel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP65sHbqRYlw",
        "outputId": "41dc554d-d988-4afe-bd19-b54aa6aaa0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disk Serialized 1x Replicated\n",
            "Serialized 1x Replicated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rdd_2.is_cached)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAC4Atd-T354",
        "outputId": "2ecadb8b-46cd-44bd-b596-9bed4db36ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_2.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41p9PDMLX6W1",
        "outputId": "968fde90-1cf1-45e2-afcc-41957a38e28f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[27] at collect at <timed eval>:1"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rdd_2.getStorageLevel())\n",
        "rdd_2.unpersist()\n",
        "print(rdd_2.getStorageLevel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHSg9LlPX-VP",
        "outputId": "b272cccd-42ae-4729-fa0c-5c500e9626e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Serialized 1x Replicated\n",
            "Serialized 1x Replicated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variables tipo Broadcast\n",
        "Permiten usar un variable (read only) cacheada en cada nodo en lugar de enviar una copia de la variable en cada tarea. Pueden ser usada por ejemplo para darle una copia a cada nodo de un dataset grande que sea usado como input."
      ],
      "metadata": {
        "id": "zhp9qMaZWajk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bc_var = sc.broadcast([1, 2, 3, 4, 5])"
      ],
      "metadata": {
        "id": "fCAKicpiWiaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[i for i in dir(bc_var) if not i.startswith(\"_\")]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN6m4qNRWraQ",
        "outputId": "87ef128d-6392-4dc1-c82a-a498dfe041bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['destroy', 'dump', 'load', 'load_from_path', 'unpersist', 'value']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bc_var"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV7kkei8YijE",
        "outputId": "54e024e5-242c-4a8e-cde3-6314de08c6aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.broadcast.Broadcast at 0x7d86b5e7a170>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bc_var.value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv7-WLgxWz9h",
        "outputId": "153d3751-cfba-4274-d314-e71b7f588d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bc_var.unpersist() # esto borrara las copias de los cache de cada executor"
      ],
      "metadata": {
        "id": "mkL7kJHiW2nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista = [1,2,3,4,5,6,7,8,9]\n",
        "suma = 0\n",
        "for num in lista:\n",
        "    suma += num\n",
        "print(suma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgLDbarjZAnR",
        "outputId": "fae6d38e-2a7d-4ae8-f581-a554f225ad63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# acumuladores...\n",
        "ac_var = sc.accumulator(0)"
      ],
      "metadata": {
        "id": "3MnuUtJkZHKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.parallelize([1,2,3,4,5,6,7,8,9]).foreach(lambda x: ac_var.add(x))"
      ],
      "metadata": {
        "id": "cAHpXBiIZMRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ac_var.value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_myzgcQzZX3v",
        "outputId": "e19b8fe4-01c7-4277-e1b8-d6457f7351a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark UI\n",
        "---\n"
      ],
      "metadata": {
        "id": "QTlw3pq4lx91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desde aqui podras monitorear el progreso de las tareas ejecutadas por spark, asi como visualizar los planes realizados por el master. Se puede visualizar en el nodo 4040 (por defecto) del nodo master. En nuestro caso lo mostraremos en el 4050. La interfaz es realmente util para hacer tunning y debugging.\n",
        "\n",
        "Normalmente si estuvieses en tu computadora podrias ver la interfaz simplemente al escribir:\n",
        "http://localhost:4040/\n",
        "\n",
        "\n",
        "En este caso ya que estamos en una instancia de Colab necesitamos usar Ngrok para enviar lo que sale del puerto 4050 a una url publica temporal."
      ],
      "metadata": {
        "id": "i2YRAUbKmy-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando Ngrok para visibilizar el SparkUI desde colab"
      ],
      "metadata": {
        "id": "6DEMacOvf8Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "with open(\"./ngrok_tunnel.py\", \"wb\") as f:\n",
        "    f.write(requests.get(\"https://raw.githubusercontent.com/engcarlosperezmolero/resources_and_tools/main/tools/ngrok_tunnel.py\").content)"
      ],
      "metadata": {
        "id": "Q2bx9SSAbIXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ngrok_tunnel import NgrokTunnel"
      ],
      "metadata": {
        "id": "bTaKrKjyezRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok = NgrokTunnel('2SnEdNgWGVppdDd5wUjVjYKCPgN_24kR77UZQ2yhgQmbhh5x4', 'linux') # se recomienda cambiar la TOKEN de NGROK api (crear cuenta)\n",
        "ngrok.download_and_unzip('https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz')\n",
        "ngrok.run_ngrok(4050)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IOl2Bjme0BO",
        "outputId": "28f74525-4001-4d79-fc1c-8cd00c5e6627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete and extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.get_public_url()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "iJbQdV8se1mR",
        "outputId": "1272a68a-40b8-4584-83a1-4a524d1723a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public url: "
          ]
        },
        {
          "output_type": "error",
          "ename": "ConnectionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    204\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    791\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             conn.request(\n\u001b[0m\u001b[1;32m    497\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             raise NewConnectionError(\n\u001b[0m\u001b[1;32m    219\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Failed to establish a new connection: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7d86b5bd0460>: Failed to establish a new connection: [Errno 111] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    845\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreason\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d86b5bd0460>: Failed to establish a new connection: [Errno 111] Connection refused'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-33ebe96a465d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_public_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/ngrok_tunnel.py\u001b[0m in \u001b[0;36mget_public_url\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_public_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Public url:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://localhost:4040/api/tunnels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"True\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"false\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"False\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"null\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tunnels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"public_url\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d86b5bd0460>: Failed to establish a new connection: [Errno 111] Connection refused'))"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leyendo un archivo usando un RDD\n",
        "---"
      ],
      "metadata": {
        "id": "rklnQ70ngGwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test.txt\n",
        "Hola que tal soy charly de humai\n",
        "Que tal charly soy Spark\n",
        "Pero a mi me gusta Python\n",
        "Genial puedes usar pyspark para comunicarte con Spark\n",
        "Usando solo Python\n",
        "Asi es usando solo Python\n",
        "Genial"
      ],
      "metadata": {
        "id": "80UtN052xHaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8102caa-e367-4c84-c37f-9c896cf8fadf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando SparkSession directamente"
      ],
      "metadata": {
        "id": "DsAocF7UrU9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines_rdd_session = spark.read.text('test.txt').rdd.map(lambda r: r[0]) # RDD[string]"
      ],
      "metadata": {
        "id": "UKyloXMrrZ0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_rdd_session.collect()"
      ],
      "metadata": {
        "id": "N1fvVzUjrte8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0237344-d053-43d4-e1dd-b516e2ac182c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hola que tal soy charly de humai',\n",
              " 'Que tal charly soy Spark',\n",
              " 'Pero a mi me gusta Python',\n",
              " 'Genial puedes usar pyspark para comunicarte con Spark',\n",
              " 'Usando solo Python',\n",
              " 'Asi es usando solo Python',\n",
              " 'Genial']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando SparkContext"
      ],
      "metadata": {
        "id": "GhDBezDJrywP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines_rdd_context = sc.textFile('test.txt')"
      ],
      "metadata": {
        "id": "ZvyiL3IL15_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_rdd_context.collect()"
      ],
      "metadata": {
        "id": "mDE3t2qEgQDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d6aa2b3-c751-4dd3-b650-78e84e23c14f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hola que tal soy charly de humai',\n",
              " 'Que tal charly soy Spark',\n",
              " 'Pero a mi me gusta Python',\n",
              " 'Genial puedes usar pyspark para comunicarte con Spark',\n",
              " 'Usando solo Python',\n",
              " 'Asi es usando solo Python',\n",
              " 'Genial']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Realizando algunas transformaciones"
      ],
      "metadata": {
        "id": "oyC95w0xr_ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = lines_rdd_context.flatMap(lambda s: s.split(' ')) # 1 a varios, primero aplica la funcion a cada elemento y luego \"aplana\" el resultado"
      ],
      "metadata": {
        "id": "-lESLfpQ7dk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_tuple = words.map(lambda word: (word, 1))"
      ],
      "metadata": {
        "id": "bVfk78v7sHNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_count = words_tuple.reduceByKey(lambda x, y: x + y)"
      ],
      "metadata": {
        "id": "IUYLsaz7znar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_count.collectAsMap()"
      ],
      "metadata": {
        "id": "2Q0o6JxTsHVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0a883f2-9a87-4cfb-9e7a-1504e6bd37ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Hola': 1,\n",
              " 'que': 1,\n",
              " 'tal': 2,\n",
              " 'soy': 2,\n",
              " 'charly': 2,\n",
              " 'de': 1,\n",
              " 'humai': 1,\n",
              " 'Que': 1,\n",
              " 'Spark': 2,\n",
              " 'Pero': 1,\n",
              " 'a': 1,\n",
              " 'mi': 1,\n",
              " 'me': 1,\n",
              " 'gusta': 1,\n",
              " 'Python': 3,\n",
              " 'Genial': 2,\n",
              " 'puedes': 1,\n",
              " 'usar': 1,\n",
              " 'pyspark': 1,\n",
              " 'para': 1,\n",
              " 'comunicarte': 1,\n",
              " 'con': 1,\n",
              " 'Usando': 1,\n",
              " 'solo': 2,\n",
              " 'Asi': 1,\n",
              " 'es': 1,\n",
              " 'usando': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kzEIIhoisHcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Si la instalación normal llega a fallar\n",
        "---"
      ],
      "metadata": {
        "id": "Lo5bA5dMRUte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "rrR2-ndLRY4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# si falla el wget buscar en la proxima pagina cual es el link que esta funcionando\n",
        "# https://spark.apache.org/downloads.html\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.0-bin-hadoop3.tgz # cambiar el nombren segun el nombre correcto del archivo"
      ],
      "metadata": {
        "id": "f7RsqPauRdRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48a6a4b3-2b37-418e-c531-c4e2cfd791fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: spark-3.3.0-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\" # cambiar el nombre segun el nombre correcto del archivo"
      ],
      "metadata": {
        "id": "dcdEOtcARdy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "w5GCl-KkReEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark # https://github.com/minrk/findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "EU1h3JrrReL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark"
      ],
      "metadata": {
        "id": "dFXPq0e3gSIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vv4uuMgVgVGE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}